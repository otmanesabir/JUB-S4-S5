---
tags: [CC]
title: 4.Time Complexity
created: '2020-05-23T17:26:26.308Z'
modified: '2020-05-24T18:58:53.672Z'
---

# 4.Time Complexity

[//]: # (Primary = #BB86FC)
[//]: # (Secondary = #03DAC5)
[//]: # (Secondary = #FF0266)

Even when a problem is decidable and this computationally solvable in principle, it may not be solvable in practice if the solution requires an inordinate amount of time or memory. This is where computational complexity thoery comes in-an investigation of the time, memory or other resources required for solving computational problems.


<h2 style="color:#BB86FC">Measuring Complexity</h2>


<h3 style="color:#03DAC5">Definition 1</h3>

Let $M$ be a deterministic Turing machine that halts on all input. The <b>running time</b> or <b>time complexity</b> of M is the function $f : \N \rightarrow \N$, where $f(n)$ is the maximum number of steps that M uses on any input of length $n$. If $f(n)$ is the running time of $M$, we say that $M$ runs in time $f(n)$ and that $M$ is an $f(n)$ time Turing machine. Customarily we use $n$ to represent the length of the input.


<h3 style="color:#03DAC5">Big-O and Small-O notation</h3>

Since the exact running time of an algorithm often is a complex expression we usually estimate it in a convenient form of estimation called <b>asymptotic analysis</b>. 


<h4 style="color:#FF0266"> Definition 2 : Big-O </h4>

Let $f$ and $g$ be function $f, g: \N \rightarrow \R^+$. Say that <b>$f(n) = O(g(n))$</b> if positive integers $c$ and $n_0$ exist such that for every integer $n \geq n_0$,

$f(n) \leq cg(n)$.

When $f(n) = O(g(n))$, we say that $g(n)$ is an <b>upper bound</b> for $f(n)$, or more precisely, that $g(n)$ is an <b>asymptotic upper bound</b> for $f(n)$, to emphasize that we are suppressing constant factors.


<h4 style="color:#FF0266"> Definition 3 : Small-O </h4>

Big-O notation has a companion called <b>small-o notation</b>. Big-O notation says that one function is asymptotically no more than another. To say that one function is asumptotically less than another, we use small-o notation. The difference between the big-O and small-O notations is analogous to the difference between $\leq$ and $<$.


Let $f$ and $g$ be function $f, g : \N \rightarrow \R^+$. Say that <b>$f(n) = o(g(n))$</b> if

$\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0$

In other words, $f(n) = o(g(n))$ means that for any real number $c > 0$, a number $n_0$ exists, where $f(n) < cg(n)$ for all $n \geq n_0$.



<h3 style="color:#03DAC5">Analyzing Algorithms</h3>

Let's analyze the TM algorithm we gave for the language $A = \{O^k 1^k | k \geq °0 \}$. 

$M_1$ = "On input string $w$:
  1. Scan across the tape and $reject$ if a 0 is found to the right of a 1.
  2. Repeat if both $0$s and $1$s remain on the tape:
    3. Scan across the tape crossing off a single 0 and a single 1.
  4. If 0s still remain after all the 1s have been crossed off, or if 1s still remain after all the 0s have been crossed off, $reject$. Otherwise, if neither 0s nor 1s remain on the tape, $accept$."

In order to analyze 1, we consider all four stages seperatly. In stage 1, the machine scans across the tape to verify that the input is of the form $0^*1^*$. Performing this scan uses $n$ steps. As we mentioned earlier, we typically use $n$ to represent the length of the input. Repositioning the head at the left-hand end of the tape uses another $n$ steps. So the total used in this stage is $2n$ steps. In big-O notation, we say that this stage uses $O(n)$ steps.

In stages 2 and 3, the machine repeatedly scanes the taps and crosses off a 0 and a 1 on each scan. Each scan uses $O(n)$ steps. Because each scan crosses off two symbols, at most $n/2$ scans can occur. So total time taken by stages two and 3 is $(n/2)O(n) = O(n^2)$ steps.

In stage 4, the machine makes a single scan to decide whether to accept or reject. The time taken in this stage us at most $O(n)$.

Thus, the total time of $M_1$ on an input of length $n$ is $O(n) + O(n^2) + O(n)$, or $O(n^2)$. In other words, its running time is $O(n^2)$, which completes the time analysis of this machine. This brings us to the next definition which provides us with a notation for classifying languages according to their time requirements.

<h4 style="color:#FF0266"> Definition 4 </h4>

Let $t : \N \rightarrow \R^+$ be a function. Define the <b>time complexity class</b>, <b>TIME(t(n))</b>, to be the collection of all languages that are decidable by an $O(t(n))$ time Turing machine.

Recall the language $A = \{O^k 1^k | k \geq °0 \}$. The preceeding analysis shows that $A \in TIME(n^2)$ because $M_1$ decides $A$ in time $O(n^2)$ and $TIME(n^2)$ contains all languages that can be decided in $O(n^2)$ time.$


<h3 style="color:#03DAC5">Complexity Relationships Among Models</h3>

Here we examine how the choice of computational model can affect the time complexity of languages. We consider three models: the single-tape Turing machine; the multitape Turing machine; and the nondeterministic Turing machine.

<h4 style="color:#FF0266"> Theorem 1 : Definition </h4>

Let $t(n)$ be a function, where $t(n) \geq n$. Then every $t(n)$ time multitape Turing machine has an equivalent $O(t^2(n))$ time single-tape Turing machine.

<h4 style="color:#FF0266"> Theorem 1 : Proof</h4>
<br>

Let $M$ be a $k-tape$ TM that runs in $t(n)$ time. We construct a single-tape TM S that runs in $O(t^2(n))$ time. <br>
Machine $S$ operate by simulating $M$. To review that simulation, we recall that $S$ uses its single tape to represent the contents on all $k$ of $M$'s tapes. The tapes are stored consecutively, with the positions of $M$'s heads marked on the appropriate squares.

Initially, $S$ puts its tape into the format that represents all the tapes of $M$ and then simulates $M$"s steps. To simulate one step, $S$ scans all the information stored on its tape to determine the symbols under $M$'s tape heads. Then $S$ makes another pass over its tape to update the tape contents and head positions. If one of $M$'s heads moves rightward onto the previously unread portion of its tape, $S$ must increase the amoount of space allocated to this tape. It does so by shifting a portion of itws taps one cell to the right.

Now we analyze this simulation. For each step of $M$, machine $S$ makes two passes over the active portion of its tape. The first obtains the infromation necessary to determine the next move and the second carries it out. The length of the active portion of $S$'s tape determins how long $S$ tkaes to scan it, so we must determine an upper bound on its length. To do so, we take the sum of lengths of the active portions of $M$'s $l$ tapes. Each of these active portions has a length of at most $t(n)$ because $M$ uses $t(n)$ tape cells in $t(n)$ steps if the head moves rightware at every step, and even fewer if a head ever moves leftward. Thus, a scan of the active portion of $S$'s tape uses $O(t(n))$ steps.

To simulate each of $M$'s steps, $S$ performs two scans and possibly up to $k$ rightward shifts. Each uses $O(t(n))$ time, so the total time for $S$ to simulate one of $M$'s steps is $O(t(n))$.

Now we bound the total time used by the simulation. The initial stage, where $S$ puts its tape into the proper format, uses $O(n)$ steps. Afterward, $S$ simulates each of the $t(n)$ steps of $M$, using $O(t(n))$ steps, so this part of the simulation uses $t(n) \times O(t(n)) = o(t^2(n))$ steps. Therefore, the entire simulation of M uses $O(n) + O(t^2(n))$ steps. 

We have assumed that $t(n) \geq n$ (a reasonable assumption because M could not even read the entire input in less time). Therefore, the running time of $S$ is $O(t^2(n))$ and the proof is complete.

---

Next, we show that any language that is decidable on such a macghine is decidable on a deterministic single-tape Turing machine that requires significantly more time.

<h4 style="color:#FF0266">Definition 5 </h4>

Let $N$ be a nondeterministic Turing machine that is a decider. The running time <b>running time</b> of N is the function $f: \N \rightarrow \N$, where $f(n)$ is the maximum number of steps that $N$ uses on any branch of its computation on any input of length $n$, as show in the following figure.


<h4 style="color:#FF0266">Theorem 2 : Definition</h4>

Let $t(n)$ be a function, where $t(n) \geq n$. Then every $t(n)$ time nondeterministic single-tape Turing machine has an equivalent $2^{O(t(n))}$ time determinstic single tape Turing machine.

<h4 style="color:#FF0266">Theorem 2 : Proof</h4>

Let $N$ be a nondeterministic TM runnning in $t(n)$ time. We construct a deterministic $TM$ $D$ that stimulates $N$ as in the proof of Theorem 3.16 by searching $N$'s nondeterministic computation tree. Now we analyze that simulation. 

On an input of length $n$, every branch of $N$'s nondeterministic computation tree has a length of at most $t(n)$. Every node in the tree can have at most $b$ children, where $b$ is the maximum number of legal choices given by $N$'s transition function. Thus, the total of leaves in the tree is at most $b^{t(n)}$.

The simulation proceeds by exploring this tree breadth first. In other words, it visits all nodes at depth $d$ before going on to any of the nodes at depth $d + 1$. The total number of nodes in the tree is less than twice the maximum number of leaves, so we bound it by $O(t(n))$. Therefore, the running time of $D$ is $O(t(n)b^{t(n)} = 2^{O(t(n))}$. Therefore, the theorem is proved.


<h2 style="color:#BB86FC">The Class P</h2>

The previous theorems illustrate an important distinction. On the one hand, we demonstrated at most a square or polynomial difference between the time complexity of problems measured on <b>deterministic single-tape and multitape Turing machines</b>. On the other hand, we showed at most an exponential difference between the time complexity of problems on <b>deterministic and nondeterministic Turing machines</b>.


<h3 style="color:#03DAC5">Polynomial Time</h3>

<h4 style="color:#FF0266">Definition 6</h4>

$P$ is the class of languages that are decidable in polynomial time on a deterministic single-tape Turing machine. In other words,
<br>
$P = \bigcup\limits_{k} TIME(n^k)$.


The class P plays a central role in our theory and is important because:

1. P is invariant for all models of computation that are polynomially equivalent to the deterministic single-tape Turing machine, and
2. P roughly corresponds to the class of problems that are reallistically solvable on a computer.


<h3 style="color:#03DAC5">Example of Problems in P</h3>

<h4 style="color:#FF0266">The Path Problem : Description</h4>

A directed graph G contains nodes $s$ and $t$, as shown in the following figure. The PATH problem is to determine whether a directed path exists from $s$ tp $t$. Let

$PATH = \{ \langle G, s, t \rangle | \text{G is a directed graph that has a directed path from s to to}\}$

---

<h4 style="color:#FF0266">The Path Problem : Proof</h4>

A polynomial time algorithm $M$ for $PATH$ operates as follows.
 
M = "On input $\langle G, s, t \rangle$, where $G$ is a directed graph with nodes $s$ and $t$:
1. Place a mark on node $s$.
2. Repeat the following until no additional nodes are marked:
  3. Scan all the edges of $G$. If an edge $(a, b)$ us found going from a marked node a to an unmarked node b, mark node b.
4. If $t$ is marked, $accept$. Otherwise, $reject$."

Let's analyze this algorithm to show that it runs in polynomial time. Obviously, stages 1 and 4 are executed only once. Stage 3 runs at most $m$ times because each time except the last it marks an additional node in $G$. Thus, the total number of stages used is at most 1 + 1 + $m$, giving a polynomial in the size of $G$.

Stages 1 and 4o of $M$ are easily implemented in polynomial time on any reasonable deterministic model. Stage $3$ involves a scan of the input and a test of whether certain nodes are marked, which is easily implemented in polynomial tiome. Hence $M$ is a polynomial time algorithm for $PATH$.


<h3 style="color:#03DAC5">Theorem 3</h3>
<h4 style="color:#FF0266">Definition</h4>

Every context-free language is a member of P.

<h4 style="color:#FF0266">Proof</h4>

We previously proved that every CFL is decidable. To do so , we gave an algorithm for each CFL that decides it. If that algorithm runs in polynomial time, the current theorem follows as a corollary. let's recall that algorithm and find out whether we can run in polynomial time.

To get a polynomial time algorithm, we introduce a powerful technique called <b>dynamic programming</b>. This technique uses the accumulation of information about smaller subproblems to solve larger problems.

In this case, we consider the subproblems of determining whether each variable in $G$ generates each substring of $w$. The algorithm enters the solution to this subproblem in $n \times n$ table. For $i \leq j$, the $(i, j)$th entry of the table contains the collection of variables that generate the substring $w_0w_1w_2w_3...w_j$. For $i > j$, the table entries are unused.

The algorithm fills in the table entries for each substring of $w$. First it fills in the entries for the substrings of length 1, then those of length 2, and so on. It uses the entries for the shorter lengths to assist in determining the entries for the longer lengths. 

For example, suppose that the algorithm has already determined which variables generate all substrings up t length $k$. To determine whether a variable $A$ generates a particular substric of length $k + 1$, the algorithm splits that substring into two nonempty pieces in the $k$ possible ways. For each split, the algorithm examines each rule $A \rightarrow BC$ to determine whether $B$ generates the first piece and $C$ generates the second piece, using table entries previously computed. If both $B$ and $C$ generate the respective pieces, $A$ generates the substring and so is added to the associated table entry. The algorithm starts the process with the string of length 1 by examining the table for the rules $A \rightarrow b$.

Now we analyze D (See Page 291). Each stage is easily implemented to run in polynomial time. Stages 4 and 5 run at most $nv$ times, where $v$ is the number of variables in $G$ and is a fixed constant independent of $n$; hence these stages run $O(n)$ times. Stage 6 runs at most $n$ times. Each time stage 6 runs, stage 7 runs at most $n$ time. Each time stage 7 runs, stage 8 runs at most $n$ times. Each time stage 9 runs, stage 10 runs $r$ times, where $r$ is the number of rules of $G$ and is another fixed constant. Thus stage 11, the inner loop of the algorithm, runs $O(n^3)$ times. Summing the total shows that $D$ executes $O(n^3)$ stages.

<h2 style="color:#BB86FC">The Class NP</h2>

<h3 style="color:#03DAC5">Verifiers</h3>
<h4 style="color:#FF0266">Definition 7</h4>

A <b>verifier</b> for a language A is an algorithm V, where

$A = \{w | V \text{accepts} \langle w, c \rangle \text{for some string c}\}.$

We measure the time of a verifier only in terms of the length of $w$, so a <b>polynomial time verifier</b> runs in polynomial time in the length of $w$. A language A is <b>polynomially verifiable</b> if it has a polynomial time verifier.

A verifier users additional information, represented by the symbol $c$, to verify that a string $w$ is a member of $A$. This information is called <b>certificate</b>, or <b>proof</b>, of a membership in A. Observe that for polynomial verifiers the certificate has polynomial length (in the length of $w$) because that is all the verifier can access in its time bound.

<h4 style="color:#FF0266">Definition 8</h4>

$NP$ is the class of languages that have polynomial time verifiers.
The class NP is important because it contains many problems of practical interest. The term NP comes from <b>nondeterministic polynomial time</b> and is derived from an alternative characteriztion by using nondeterministic polynomial time Turing machines. Problems in NP are sometimes called NP-problems.


<h4 style="color:#FF0266">Theorem 4 : Definition</h4>

A language is in NP iff it is decided by some nondeterministic polynomial time Turing machine.

<h4 style="color:#FF0266">Theorem 4 : Proof</h4>

The idea behind the proof is that we show how to convert a polynomial time verifier to an equivalent polynomial time NTM and vice versa. The NTM simulates the verifier by guessing the certificate. The verifier simulates the NTM by using teh accepting branch as a certificate
<br>
For the forward direction of this theorem, let $A \in NP$ and show tjhat A is decided by a polynomial time NTM $N$. Let $V$ be the polynomial time verifier for that $A$ that exists by the definition of $NP$. Assume that $V$ is a $TM$ that runs in time $n^k$ and construct $N$ as follows.

N = "On input $w$ of length $n$:
  1. Nondeterministically select string $c$ of length at most $n^k$.
  2. Run $V$ on input $\langle w, c \rangle$.
  3. If $V$ accepts, $accept$; otherwise, $reject$."

To prove the other direction of the theorem, assume that $A$ is decided by a polynomial time NTM $N$ and construct a polynomial time verifier $V$ as follows.

V = "On input $\langle w, c \rangle$, where $w$ and $c$ are strings:
  1. Simulte $N$ on input $w$, treating each symbol of $c$ as a description of the nondeterministic choice to make at each step.
  2. If this branch of $N$'s computation accepts, $accpet$; otherwise, $reject$."

---

We define the nondeterministic time complexity class $NTIME(t(n))$ as analogous to the deterministic time complexity class $TIME(t(n))$.

<h4 style="color:#FF0266">Definition 9</h4>

<b>$NTIME(t(n))$</b> $= \{ L | L \text{ is a language decided by an } O(t(n)) \text{ time nondeterministic Turing machine} \}.$ 

<h4 style="color:#FF0266">Corollary</h4>

$NP = \bigcup\limits_{k} NTIME(n^k)$

The class NP is insensitive to the choice of reasonable nondeterministic computational model because all such models are polynomially equivalent. When describing and anaylizing nondeterministic  polynomial time algorithms, we follow the preceding conventions for deterministic polynomial time algorithms.

<h3 style="color:#03DAC5">Example of Problems in NP</h3>

<h4 style="color:#FF0266">Clique</h4>

A <b>clique</b> in an undirected graph is a subgraph, wherein every two nodes are connected by an edge. A $k-clique$ is a clique that contains $k$ nodes. The clique problem is to determine whether a graph contains a clique of a specified size. Let

$CLIQUE = \{ \langle G, k \rangle | G \text{is an undirected graph with a k-clique}\}.$


<h4 style="color:#FF0266">Clique : Proof</h4>

The following verifier $V$ for $CLIQUE$.
V = "On input $\langle \langle G, k \rangle, c\rangle$:
  1. Test whether $c$ is a subgraph with $k$ nodes in $G$.
  2. Test whether $G$ contains all edges connecting nodes in $c$.
  3. If both pass, $accept$; otherwise, $reject$."


Observe that the complement of this set $\overline{CLIQUE}$ is not obviously a member of NP. Verifying that something is _not_ present seems to be more difficult than verifying that it _is_ present. We make a seperate complexity class, called **coNP**, which contains the languages that are complements of languages in NP. We don't know whether **coNP** is different from NP.


<h3 style="color:#03DAC5">The P vs NP Question</h3>

As we have been saying, NP is the class of languages that are solvable in polynomial time on a nondeterministic Turing machine; or, equivalently, it is the class of languages whereby membership in the language can be verified in polynomial time. P is the class of languages where membership can be tested in polynomial time. We summarize this information as follows, where we loosely refer to polynomial time solvable as solvable "quickly".


P = the class of languages for which membership can be _decided_ quickly.
NP = the class of languages for which membership can be _verified_ quicky.

The power of polynomial verifiability seems to be much greater than that of polynomial decidability. But, hard as it may be to imagine, P and NP could be equal. We are unable to _prove_ the existence of a single language in NP that is not in P.


<h2 style="color:#BB86FC">NP-Completeness</h2>

One important advance on the P versus NP question came in the early 1970s with the work of Stephen Cook and Leonid Levin. They discovered certain problems in NP whose individual complexity is related to that of the entire class. If a polynomial time algorithm exists for any of these problems, all problems in NP would be polynomial time solvable. These problems are called **NP-complete**.
The phenomenon of NP-completeness is important for both theoretical and practical reasons.

In theory, a researcher trying to show that P is unequal to NP may focus on a NP-complete problem. If any problem in NP requires more than polynomial time, an NP-complete one does. Furthermore, a researcher attempting to prove that P equals NP only needs to find a polynomial time algorithm for an NP-complete problem to achieve this goal.

In practice, the phenomenon of NP-completeness may prevent wasting time searching for a nonexistent polynomial time algorithm to solve a particular problem. Even though we may not have the necessary mathematics to prove that the problem is unsolvable in polynomial time, we believe that P is unequal to NP. So proving that a problem NP-complete is strong evident of its nonpolynomiality.

The first NP-complete problem we'll see if the infamous **satisfiability problem**. Recall that a boolean formula is an expression involving Boolean variables and operations. A boolean formula is **satisfiable** if some assignment of 0s and 1s to the variables makes the formula evaluate to 1. The **satisfiability problem** is to test whether a Boolean formula is satisfiable. Let

$SAT = \{ \langle \phi \rangle \ | \phi \text{ is a satisfiable Boolean formula} \}.$

<h4 style="color:#FF0266">Theorem 5</h4>

$SAT \in P$ iff $P = NP$

In order to prove this theorem we first need to understand certain concepts.

--- 

<h3 style="color:#03DAC5">Polynomial Time Reducibility</h3>

We previously defined the concept of reducing one problem to another by the statement that when a problem A reduces to problem B, a solution B can be used to solve A. Now we define a version of reducibility that takes the efficiency of computation into account. When problem A is _efficiently_ reducible to problem B, an efficient solution to B can be used to solve A efficiently.

<h4 style="color:#FF0266">Definition 10</h4>

A function $f : \Sigma^* \rightarrow \Sigma^*$ is a <b>polynomial time computable function</b> if some polynomial time Turing machine M exists that halts with just $f(w)$ on its tape, when started on any input $w$.

<h4 style="color:#FF0266">Definition 11</h4>

Language A is <b>polynomial time mapping reducible</b>, or simple <b>polynomial time reducible</b>, to language B, written $A \leq_p B$ if a polynomial time computable function $f : \Sigma^* \rightarrow \Sigma^*$ exists, where for every $w$,

$w \in A \iff f(w) \in B$

The function $f$ is called the <b>polynomial time reduction</b> of A to B.

<h4 style="color:#FF0266">Theorem 7 : Definition</h4>

If $A \leq_P B and B \in P$, then $A \in P$.

<h4 style="color:#FF0266">Theorem 7 : Proof</h4>

Let M be the polynomial time algorithm deciding B and $f$ be the polynomial time reduction from A to B. We describe a polynomial time algorithm $N$ deciding $A$ as follows.

N = "On input $w$:
  1. Compute $f(w)$
  2. Run $M$ on input $f(w)$ and output whatever $M$ outputs."

We have $w \in A$ whenever $f(w) \in B$ because $f$ is a reduction from A to B. Thus, $M$ accepts $f(w)$ whenever $w \in A$. Moreover, $N$ runs in polynomial time because each of its two stages runs in polynomial time. Note that stage 2 runs in polynomial time because the composition of two polynomials is a polynomial. 

--- 

Before demonstrating a polynomial time reduction, we introduce $3SAT$, a special case of the satisfiability problem whereby all formulas are in a special form - the conjuctive normal form. <b>The conjuctive normal form </b>, called a <b>cnf-formula</b>, is a boolean clause which comprises of several clauses connected with $\land$'s, as in

$(x_1 \lor \overline{x_2} \lor \overline{x_3} \lor x_4) \land (x_3 \lor \overline{x_5} \lor x_6) \lor (x_3 \lor \overline{x_6})$

It is a <b>3cnf-formula</b> if all the clauses have three literals, as in

$(x_1 \lor \overline{x_2} \lor \overline{x_3}) \land (x_3 \lor \overline{x_5} \lor x_6) \land (x_3 \lor \overline{x_6} \lor x_4)$


Let $3SAT = \{\langle \phi \rangle | \phi \text{ is a satisfiable 3cnf-formula} \}$. If an assignment satisfies a cnf-formula, each clause must contain at least one literal that evaluates to 1.

The following theorem presents a polynomial time reduction from the $3SAT$ problem to the $CLIQUE$ problem.


... ADD PROOF?

<h3 style="color:#03DAC5">Definition of NP-Completeness</h3>
<h4 style="color:#FF0266">Definition 12</h4>

A language B is **NP-complete** if it satisfies two conditions:
  1. B is in NP, and
  2. every A in NP is polynomial time reducible to B.

<h4 style="color:#FF0266">Theorem 8 : Definition</h4>

If $B$ is $NP-complete$ and $B \in P$, then $P = NP$.

<h4 style="color:#FF0266">Theorem 8 : Proof</h4>

This theorem follows directly from the definition of polynomial time reducibility.

<h4 style="color:#FF0266">Theorem 9 : Definition</h4>

If $B$ is $NP-complete$ and $B \leq_p C$ for $C$ in $NP$, then $C$ is NP-complete.

<h4 style="color:#FF0266">Theorem 9 : Proof</h4>

We already know that C is in NP, so we must show that every A in NP is time reducible to C. Because B is NP-complete, every language in NP is polynomial time reducible to B, and B in turn is polynomial time reducible to C. Polynomial time reductions compose; that is, if A is polynomial time reducible to B and B is polynomial time reducible to C, then A is polynomial time reducible to C. Hence every language in NP is polynomial time reducible to C.

<h3 style="color:#03DAC5">The Cook-Levin Theorem</h3>

Once we have one NP-complete problem, we may obtain others by polynomial time reduction from it. However, establishing the first NP-complete problem is more difficult. Now we do so by proving that SAT is NP-complete.
