{"cells":[{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00000-5ecfde59-5d5e-4534-b170-da0e4f2b5ace","output_cleared":false,"source_hash":"66f71ed9","execution_millis":32338,"execution_start":1606732247671},"source":"%%bash\n$HADOOP_HOME/sbin/start-dfs.sh\n$HADOOP_HOME/sbin/start-yarn.sh\n$HADOOP_HOME/bin/mapred --daemon start historyserver","execution_count":1,"outputs":[{"name":"stdout","text":"Starting namenodes on [localhost]\nStarting datanodes\nStarting secondary namenodes [p-ec4ea129-ae6a-44e4-85a6-40ee24c22258]\nStarting resourcemanager\nStarting nodemanagers\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00001-f6c288fb-cc2a-4dba-8e14-7b88044f7b30","output_cleared":false,"source_hash":"aad242c9","execution_millis":12525,"execution_start":1606732280021},"source":"! $HADOOP_HOME/bin/hdfs dfs -put salaries.csv","execution_count":2,"outputs":[{"name":"stdout","text":"put: `salaries.csv': File exists\r\n","output_type":"stream"}]},{"cell_type":"code","source":"from hdfs3 import HDFileSystem\nhdfs = HDFileSystem(host='localhost', port=9000)\nhdfs.ls('.')","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00002-9616b1d5-5233-41c0-be34-a29d86476a52","output_cleared":false,"source_hash":"72dab2b2","execution_millis":133,"execution_start":1606732292610},"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"['/user/jovyan/.sparkStaging',\n '/user/jovyan/output',\n '/user/jovyan/output2',\n '/user/jovyan/salaries.csv']"},"metadata":{}}],"execution_count":3},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00001-6679f9eb-4e83-4b53-a22f-078fc6412cd8","output_cleared":false,"source_hash":"ce43cb53","execution_millis":0,"execution_start":1606732675391},"source":"%%file salaries_spark.py\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import expr\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom functools import reduce\n\n# SAMPLE LINE:\n#\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  ,06/10/2013,$11310.00,$873.63\n\nspark = SparkSession.builder.appName(\"Top Salaries\").getOrCreate()\n\nmySchema = StructType([\n    StructField(\"a\", StringType(), False),\n    StructField(\"b\", StringType(), False),\n    StructField(\"c\", StringType(), False),\n    StructField(\"d\", StringType(), False),\n    StructField(\"e\", StringType(), False),\n    StructField(\"salary\", StringType(), False),\n    StructField(\"f\", StringType(), False)\n    ])\n\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").schema(mySchema).load(\"salaries.csv\")\nexcluColumns = \"a,b,c,d,e,f\".split(\",\")\ndf = reduce(DataFrame.drop, excluColumns, df).withColumn('d_salary', regexp_replace('salary', '[$,]', '').cast('double'))\nexcluColumns= [\"salary\"]\ndf2 = reduce(DataFrame.drop, excluColumns, df)\ndf2.show()\nresult = df2.sort(desc(\"d_salary\")).limit(10)\nresult.write.format(\"csv\").option(\"path\", \"output3\").save()\n\n","execution_count":6,"outputs":[{"name":"stdout","text":"Overwriting salaries_spark.py\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00002-6a73e47e-7814-42fe-975c-ffafbeb6a5b9","output_cleared":false,"source_hash":"8b9db27a","execution_millis":41493,"execution_start":1606732676976},"source":"! $SPARK_HOME/bin/spark-submit --master yarn salaries_spark.py","execution_count":7,"outputs":[{"name":"stdout","text":"WARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/jovyan/spark-3.0.1-bin-without-hadoop/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n2020-11-30 10:37:59,644 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2020-11-30 10:38:00,624 INFO spark.SparkContext: Running Spark version 3.0.1\n2020-11-30 10:38:00,709 INFO resource.ResourceUtils: ==============================================================\n2020-11-30 10:38:00,713 INFO resource.ResourceUtils: Resources for spark.driver:\n\n2020-11-30 10:38:00,713 INFO resource.ResourceUtils: ==============================================================\n2020-11-30 10:38:00,715 INFO spark.SparkContext: Submitted application: Top Salaries\n2020-11-30 10:38:00,819 INFO spark.SecurityManager: Changing view acls to: jovyan\n2020-11-30 10:38:00,820 INFO spark.SecurityManager: Changing modify acls to: jovyan\n2020-11-30 10:38:00,820 INFO spark.SecurityManager: Changing view acls groups to: \n2020-11-30 10:38:00,820 INFO spark.SecurityManager: Changing modify acls groups to: \n2020-11-30 10:38:00,820 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n2020-11-30 10:38:01,208 INFO util.Utils: Successfully started service 'sparkDriver' on port 38585.\n2020-11-30 10:38:01,237 INFO spark.SparkEnv: Registering MapOutputTracker\n2020-11-30 10:38:01,309 INFO spark.SparkEnv: Registering BlockManagerMaster\n2020-11-30 10:38:01,327 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n2020-11-30 10:38:01,328 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n2020-11-30 10:38:01,411 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n2020-11-30 10:38:01,428 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-7b9fbdda-bd3f-4b91-9e00-2ead1d4ff06a\n2020-11-30 10:38:01,454 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB\n2020-11-30 10:38:01,531 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n2020-11-30 10:38:01,729 INFO util.log: Logging initialized @4003ms to org.sparkproject.jetty.util.log.Slf4jLog\n2020-11-30 10:38:01,786 INFO server.Server: jetty-9.4.z-SNAPSHOT; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 11.0.9+11-post-Debian-1deb10u1\n2020-11-30 10:38:01,819 INFO server.Server: Started @4093ms\n2020-11-30 10:38:01,849 INFO server.AbstractConnector: Started ServerConnector@3235ef76{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n2020-11-30 10:38:01,849 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n2020-11-30 10:38:01,906 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@464ceef2{/jobs,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,909 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72fdde1e{/jobs/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,910 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@275a924c{/jobs/job,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,913 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@245469ff{/jobs/job/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,914 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1073b244{/stages,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,914 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58b4ec0b{/stages/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,915 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45d8da3b{/stages/stage,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,917 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c2c22bf{/stages/stage/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,918 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3fe71c7f{/stages/pool,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,919 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@548f414b{/stages/pool/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,920 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50ad5701{/storage,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,921 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7af1c9b5{/storage/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,922 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6239871d{/storage/rdd,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,923 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10d654ce{/storage/rdd/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,924 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50334d7a{/environment,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,925 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@624374d0{/environment/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,926 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37b42ce4{/executors,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,927 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7311c33{/executors/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,928 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b2d87f9{/executors/threadDump,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,929 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@42df78cb{/executors/threadDump/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,937 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a90708d{/static,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,938 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7089276a{/,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,939 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f56d0e{/api,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,940 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4403fb04{/jobs/job/kill,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,941 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23516aac{/stages/stage/kill,null,AVAILABLE,@Spark}\n2020-11-30 10:38:01,943 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:4040\n2020-11-30 10:38:02,446 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n2020-11-30 10:38:02,966 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\n2020-11-30 10:38:03,961 INFO conf.Configuration: resource-types.xml not found\n2020-11-30 10:38:03,962 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n2020-11-30 10:38:04,010 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n2020-11-30 10:38:04,011 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n2020-11-30 10:38:04,011 INFO yarn.Client: Setting up container launch context for our AM\n2020-11-30 10:38:04,013 INFO yarn.Client: Setting up the launch environment for our AM container\n2020-11-30 10:38:04,018 INFO yarn.Client: Preparing resources for our AM container\n2020-11-30 10:38:04,053 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n2020-11-30 10:38:04,744 INFO yarn.Client: Uploading resource file:/tmp/spark-8f7dc1e1-3880-499b-8f28-a3e6238714fa/__spark_libs__8170324852134862418.zip -> hdfs://localhost:9000/user/jovyan/.sparkStaging/application_1606732279220_0002/__spark_libs__8170324852134862418.zip\n2020-11-30 10:38:06,215 INFO yarn.Client: Uploading resource file:/home/jovyan/spark-3.0.1-bin-without-hadoop/python/lib/pyspark.zip -> hdfs://localhost:9000/user/jovyan/.sparkStaging/application_1606732279220_0002/pyspark.zip\n2020-11-30 10:38:06,319 INFO yarn.Client: Uploading resource file:/home/jovyan/spark-3.0.1-bin-without-hadoop/python/lib/py4j-0.10.9-src.zip -> hdfs://localhost:9000/user/jovyan/.sparkStaging/application_1606732279220_0002/py4j-0.10.9-src.zip\n2020-11-30 10:38:06,811 INFO yarn.Client: Uploading resource file:/tmp/spark-8f7dc1e1-3880-499b-8f28-a3e6238714fa/__spark_conf__142517021450516163.zip -> hdfs://localhost:9000/user/jovyan/.sparkStaging/application_1606732279220_0002/__spark_conf__.zip\n2020-11-30 10:38:06,912 INFO spark.SecurityManager: Changing view acls to: jovyan\n2020-11-30 10:38:06,912 INFO spark.SecurityManager: Changing modify acls to: jovyan\n2020-11-30 10:38:06,912 INFO spark.SecurityManager: Changing view acls groups to: \n2020-11-30 10:38:06,913 INFO spark.SecurityManager: Changing modify acls groups to: \n2020-11-30 10:38:06,913 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n2020-11-30 10:38:06,955 INFO yarn.Client: Submitting application application_1606732279220_0002 to ResourceManager\n2020-11-30 10:38:07,259 INFO impl.YarnClientImpl: Submitted application application_1606732279220_0002\n2020-11-30 10:38:08,265 INFO yarn.Client: Application report for application_1606732279220_0002 (state: ACCEPTED)\n2020-11-30 10:38:08,268 INFO yarn.Client: \n\t client token: N/A\n\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: default\n\t start time: 1606732687023\n\t final status: UNDEFINED\n\t tracking URL: http://p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:8088/proxy/application_1606732279220_0002/\n\t user: jovyan\n2020-11-30 10:38:09,272 INFO yarn.Client: Application report for application_1606732279220_0002 (state: ACCEPTED)\n2020-11-30 10:38:10,310 INFO yarn.Client: Application report for application_1606732279220_0002 (state: ACCEPTED)\n2020-11-30 10:38:11,314 INFO yarn.Client: Application report for application_1606732279220_0002 (state: ACCEPTED)\n2020-11-30 10:38:12,316 INFO yarn.Client: Application report for application_1606732279220_0002 (state: RUNNING)\n2020-11-30 10:38:12,316 INFO yarn.Client: \n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: 172.2.140.18\n\t ApplicationMaster RPC port: -1\n\t queue: default\n\t start time: 1606732687023\n\t final status: UNDEFINED\n\t tracking URL: http://p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:8088/proxy/application_1606732279220_0002/\n\t user: jovyan\n2020-11-30 10:38:12,319 INFO cluster.YarnClientSchedulerBackend: Application application_1606732279220_0002 has started running.\n2020-11-30 10:38:12,348 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39575.\n2020-11-30 10:38:12,348 INFO netty.NettyBlockTransferService: Server created on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:39575\n2020-11-30 10:38:12,350 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n2020-11-30 10:38:12,416 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, p-ec4ea129-ae6a-44e4-85a6-40ee24c22258, 39575, None)\n2020-11-30 10:38:12,448 INFO storage.BlockManagerMasterEndpoint: Registering block manager p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:39575 with 434.4 MiB RAM, BlockManagerId(driver, p-ec4ea129-ae6a-44e4-85a6-40ee24c22258, 39575, None)\n2020-11-30 10:38:12,463 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, p-ec4ea129-ae6a-44e4-85a6-40ee24c22258, 39575, None)\n2020-11-30 10:38:12,464 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, p-ec4ea129-ae6a-44e4-85a6-40ee24c22258, 39575, None)\n2020-11-30 10:38:13,011 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57bddaab{/metrics/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:13,051 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> p-ec4ea129-ae6a-44e4-85a6-40ee24c22258, PROXY_URI_BASES -> http://p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:8088/proxy/application_1606732279220_0002), /proxy/application_1606732279220_0002\n2020-11-30 10:38:14,610 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n2020-11-30 10:38:20,530 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n2020-11-30 10:38:23,741 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.2.140.18:59750) with ID 1\n2020-11-30 10:38:24,343 INFO storage.BlockManagerMasterEndpoint: Registering block manager p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:41407 with 434.4 MiB RAM, BlockManagerId(1, p-ec4ea129-ae6a-44e4-85a6-40ee24c22258, 41407, None)\n2020-11-30 10:38:25,728 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.2.140.18:60268) with ID 2\n2020-11-30 10:38:25,823 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n2020-11-30 10:38:26,050 INFO storage.BlockManagerMasterEndpoint: Registering block manager p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:34235 with 434.4 MiB RAM, BlockManagerId(2, p-ec4ea129-ae6a-44e4-85a6-40ee24c22258, 34235, None)\n2020-11-30 10:38:26,242 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/work/spark-warehouse').\n2020-11-30 10:38:26,243 INFO internal.SharedState: Warehouse path is 'file:/home/jovyan/work/spark-warehouse'.\n2020-11-30 10:38:26,257 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n2020-11-30 10:38:26,260 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@313d7026{/SQL,null,AVAILABLE,@Spark}\n2020-11-30 10:38:26,260 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n2020-11-30 10:38:26,262 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2153af6d{/SQL/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:26,262 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n2020-11-30 10:38:26,263 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@553b750b{/SQL/execution,null,AVAILABLE,@Spark}\n2020-11-30 10:38:26,264 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n2020-11-30 10:38:26,265 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29ed2d2{/SQL/execution/json,null,AVAILABLE,@Spark}\n2020-11-30 10:38:26,266 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n2020-11-30 10:38:26,267 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cca82b6{/static/sql,null,AVAILABLE,@Spark}\n2020-11-30 10:38:27,292 INFO datasources.InMemoryFileIndex: It took 46 ms to list leaf files for 1 paths.\n2020-11-30 10:38:30,012 INFO datasources.FileSourceStrategy: Pruning directories with: \n2020-11-30 10:38:30,015 INFO datasources.FileSourceStrategy: Pushed Filters: \n2020-11-30 10:38:30,016 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n2020-11-30 10:38:30,018 INFO datasources.FileSourceStrategy: Output Data Schema: struct<salary: string>\n2020-11-30 10:38:30,872 INFO codegen.CodeGenerator: Code generated in 355.824342 ms\n2020-11-30 10:38:30,929 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 299.0 KiB, free 434.1 MiB)\n2020-11-30 10:38:31,018 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 50.8 KiB, free 434.1 MiB)\n2020-11-30 10:38:31,020 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:39575 (size: 50.8 KiB, free: 434.4 MiB)\n2020-11-30 10:38:31,023 INFO spark.SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0\n2020-11-30 10:38:31,047 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n2020-11-30 10:38:31,245 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n2020-11-30 10:38:31,259 INFO scheduler.DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n2020-11-30 10:38:31,259 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n2020-11-30 10:38:31,259 INFO scheduler.DAGScheduler: Parents of final stage: List()\n2020-11-30 10:38:31,260 INFO scheduler.DAGScheduler: Missing parents: List()\n2020-11-30 10:38:31,310 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n2020-11-30 10:38:31,461 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.8 KiB, free 434.0 MiB)\n2020-11-30 10:38:31,464 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.0 MiB)\n2020-11-30 10:38:31,506 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:39575 (size: 6.8 KiB, free: 434.3 MiB)\n2020-11-30 10:38:31,507 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223\n2020-11-30 10:38:31,521 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n2020-11-30 10:38:31,522 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\n2020-11-30 10:38:31,552 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, p-ec4ea129-ae6a-44e4-85a6-40ee24c22258, executor 2, partition 0, NODE_LOCAL, 7754 bytes)\n2020-11-30 10:38:32,038 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:34235 (size: 6.8 KiB, free: 434.4 MiB)\n2020-11-30 10:38:33,655 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:34235 (size: 50.8 KiB, free: 434.3 MiB)\n2020-11-30 10:38:34,943 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3398 ms on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258 (executor 2) (1/1)\n2020-11-30 10:38:34,945 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n2020-11-30 10:38:34,951 INFO scheduler.DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 3.627 s\n2020-11-30 10:38:34,955 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n2020-11-30 10:38:34,955 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\n2020-11-30 10:38:34,957 INFO scheduler.DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 3.712286 s\n2020-11-30 10:38:35,047 INFO codegen.CodeGenerator: Code generated in 24.192613 ms\n+--------+\n|d_salary|\n+--------+\n| 53428.0|\n| 68300.0|\n| 62000.0|\n| 43999.0|\n| 52000.0|\n| 62175.0|\n| 70918.0|\n| 42438.0|\n| 11310.0|\n| 11310.0|\n| 11310.0|\n| 34692.0|\n| 33215.0|\n| 11310.0|\n| 41202.0|\n| 28802.0|\n| 37199.0|\n| 44190.0|\n| 11310.0|\n| 20800.0|\n+--------+\nonly showing top 20 rows\n\n2020-11-30 10:38:35,433 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:34235 in memory (size: 6.8 KiB, free: 434.4 MiB)\n2020-11-30 10:38:35,442 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:39575 in memory (size: 6.8 KiB, free: 434.4 MiB)\n2020-11-30 10:38:35,537 INFO datasources.FileSourceStrategy: Pruning directories with: \n2020-11-30 10:38:35,537 INFO datasources.FileSourceStrategy: Pushed Filters: \n2020-11-30 10:38:35,537 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n2020-11-30 10:38:35,537 INFO datasources.FileSourceStrategy: Output Data Schema: struct<salary: string>\n2020-11-30 10:38:35,640 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n2020-11-30 10:38:35,640 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n2020-11-30 10:38:35,642 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n2020-11-30 10:38:35,663 INFO codegen.CodeGenerator: Code generated in 11.511349 ms\n2020-11-30 10:38:35,732 INFO codegen.CodeGenerator: Code generated in 22.82296 ms\n2020-11-30 10:38:35,740 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 299.0 KiB, free 433.8 MiB)\n2020-11-30 10:38:35,812 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 50.8 KiB, free 433.7 MiB)\n2020-11-30 10:38:35,813 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:39575 (size: 50.8 KiB, free: 434.3 MiB)\n2020-11-30 10:38:35,814 INFO spark.SparkContext: Created broadcast 2 from save at NativeMethodAccessorImpl.java:0\n2020-11-30 10:38:35,815 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n2020-11-30 10:38:35,943 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n2020-11-30 10:38:35,946 INFO scheduler.DAGScheduler: Registering RDD 9 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n2020-11-30 10:38:35,948 INFO scheduler.DAGScheduler: Got job 1 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n2020-11-30 10:38:35,948 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (save at NativeMethodAccessorImpl.java:0)\n2020-11-30 10:38:35,948 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n2020-11-30 10:38:35,949 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n2020-11-30 10:38:35,950 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n2020-11-30 10:38:36,010 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.5 KiB, free 433.7 MiB)\n2020-11-30 10:38:36,012 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 433.7 MiB)\n2020-11-30 10:38:36,012 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:39575 (size: 10.4 KiB, free: 434.3 MiB)\n2020-11-30 10:38:36,013 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223\n2020-11-30 10:38:36,016 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n2020-11-30 10:38:36,016 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\n2020-11-30 10:38:36,018 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, p-ec4ea129-ae6a-44e4-85a6-40ee24c22258, executor 2, partition 0, NODE_LOCAL, 7743 bytes)\n2020-11-30 10:38:36,107 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:34235 (size: 10.4 KiB, free: 434.3 MiB)\n2020-11-30 10:38:36,413 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:34235 (size: 50.8 KiB, free: 434.3 MiB)\n2020-11-30 10:38:36,933 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 916 ms on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258 (executor 2) (1/1)\n2020-11-30 10:38:36,935 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (save at NativeMethodAccessorImpl.java:0) finished in 0.981 s\n2020-11-30 10:38:36,936 INFO scheduler.DAGScheduler: looking for newly runnable stages\n2020-11-30 10:38:36,936 INFO scheduler.DAGScheduler: running: Set()\n2020-11-30 10:38:36,936 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)\n2020-11-30 10:38:36,937 INFO scheduler.DAGScheduler: failed: Set()\n2020-11-30 10:38:36,939 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n2020-11-30 10:38:36,940 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n2020-11-30 10:38:37,014 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 296.7 KiB, free 433.4 MiB)\n2020-11-30 10:38:37,032 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 110.7 KiB, free 433.3 MiB)\n2020-11-30 10:38:37,032 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:39575 (size: 110.7 KiB, free: 434.2 MiB)\n2020-11-30 10:38:37,033 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223\n2020-11-30 10:38:37,034 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n2020-11-30 10:38:37,034 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\n2020-11-30 10:38:37,036 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, p-ec4ea129-ae6a-44e4-85a6-40ee24c22258, executor 2, partition 0, NODE_LOCAL, 7336 bytes)\n2020-11-30 10:38:37,111 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:34235 (size: 110.7 KiB, free: 434.2 MiB)\n2020-11-30 10:38:37,329 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.2.140.18:60268\n2020-11-30 10:38:37,756 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 721 ms on p-ec4ea129-ae6a-44e4-85a6-40ee24c22258 (executor 2) (1/1)\n2020-11-30 10:38:37,757 INFO scheduler.DAGScheduler: ResultStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 0.812 s\n2020-11-30 10:38:37,758 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n2020-11-30 10:38:37,758 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \n2020-11-30 10:38:37,758 INFO cluster.YarnScheduler: Killing all running tasks in stage 2: Stage finished\n2020-11-30 10:38:37,759 INFO scheduler.DAGScheduler: Job 1 finished: save at NativeMethodAccessorImpl.java:0, took 1.815308 s\n2020-11-30 10:38:37,822 INFO datasources.FileFormatWriter: Write Job 1c13a2bf-b5cd-407c-a2b2-eacd6da919f6 committed.\n2020-11-30 10:38:37,824 INFO datasources.FileFormatWriter: Finished processing stats for write job 1c13a2bf-b5cd-407c-a2b2-eacd6da919f6.\n2020-11-30 10:38:37,864 INFO spark.SparkContext: Invoking stop() from shutdown hook\n2020-11-30 10:38:37,869 INFO server.AbstractConnector: Stopped Spark@3235ef76{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n2020-11-30 10:38:37,870 INFO ui.SparkUI: Stopped Spark web UI at http://p-ec4ea129-ae6a-44e4-85a6-40ee24c22258:4040\n2020-11-30 10:38:37,874 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\n2020-11-30 10:38:37,927 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\n2020-11-30 10:38:37,928 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n2020-11-30 10:38:37,939 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\n2020-11-30 10:38:37,953 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n2020-11-30 10:38:38,022 INFO memory.MemoryStore: MemoryStore cleared\n2020-11-30 10:38:38,023 INFO storage.BlockManager: BlockManager stopped\n2020-11-30 10:38:38,026 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n2020-11-30 10:38:38,032 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n2020-11-30 10:38:38,137 INFO spark.SparkContext: Successfully stopped SparkContext\n2020-11-30 10:38:38,138 INFO util.ShutdownHookManager: Shutdown hook called\n2020-11-30 10:38:38,138 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8f7dc1e1-3880-499b-8f28-a3e6238714fa/pyspark-fe8ff817-bb76-48e5-932f-2e2bc87a5866\n2020-11-30 10:38:38,213 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-cbf4c172-d39e-42a1-9125-8214a8329062\n2020-11-30 10:38:38,216 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8f7dc1e1-3880-499b-8f28-a3e6238714fa\n","output_type":"stream"}]},{"cell_type":"code","source":"! $HADOOP_HOME/bin/hdfs dfs -get output3 .","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-1c8f2236-dc9a-454e-b171-f9379966a448","output_cleared":false,"source_hash":"f11f2bae","execution_millis":3466,"execution_start":1606732718472},"outputs":[],"execution_count":8},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00003-cee65112-8e33-4198-a18b-c2d6a16af25d","output_cleared":true,"source_hash":null,"execution_millis":8018,"execution_start":1605110810785},"source":"%%bash\n$HADOOP_HOME/bin/mapred --daemon stop historyserver\n$HADOOP_HOME/sbin/stop-yarn.sh\n$HADOOP_HOME/sbin/stop-dfs.sh","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"6acb2c26-ddc8-48ad-85cb-e025e400398c","deepnote_execution_queue":[]}}